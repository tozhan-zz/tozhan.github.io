<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on </title>
    <link>https://tozhan.github.io/tags/math/</link>
    <description>Recent content in math on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Feb 2020 13:26:17 -0800</lastBuildDate>
    
	<atom:link href="https://tozhan.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient</title>
      <link>https://tozhan.github.io/post/gradient/</link>
      <pubDate>Sat, 22 Feb 2020 13:26:17 -0800</pubDate>
      
      <guid>https://tozhan.github.io/post/gradient/</guid>
      <description>Introduction What is Gradient  One-dimensional Independent Variable Two-dimensional Independent Variables Multi-dimensional Independent Variables   Gradient in Machine Learning  Gradient Descent Three Types of Gradient Descent   Conclusion  Introduction Gradient descent is widely used in machine learning for parameters optimization. If you have heard of gradient many times, but still wonder what is behind it, then you are in the right place. This page will go through:</description>
    </item>
    
  </channel>
</rss>